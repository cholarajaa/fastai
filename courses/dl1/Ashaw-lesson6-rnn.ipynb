{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from fastai.io import *\n",
    "from fastai.conv_learner import *\n",
    "\n",
    "from fastai.column_data import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to download the collected works of Nietzsche to use as our data for this class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH='data/nietzsche/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nietzsche.txt: 606KB [00:00, 649KB/s]                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 600893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    }
   ],
   "source": [
    "get_data(\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\", f'{PATH}nietzsche.txt')\n",
    "text = open(f'{PATH}nietzsche.txt').read()\n",
    "print('corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PREFACE\\n\\n\\nSUPPOSING that Truth is a woman--what then? Is there not ground\\nfor suspecting that all philosophers, in so far as they have been\\ndogmatists, have failed to understand women--that the terrible\\nseriousness and clumsy importunity with which they have usually paid\\ntheir addresses to Truth, have been unskilled and unseemly methods for\\nwinning a woman? Certainly she has never allowed herself '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 85\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)+1\n",
    "print('total chars:', vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it's useful to have a zero value in the dataset, e.g. for padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n !\"\\'(),-.0123456789:;=?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_abcdefghijklmnopqrstuvwxy'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars.insert(0, \"\\0\")\n",
    "\n",
    "''.join(chars[1:-6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map from chars to indices and back again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*idx* will be the data we use from now own - it simply converts all the characters to their index (based on the mapping above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[40, 42, 29, 30, 25, 27, 29, 1, 1, 1]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = [char_indices[c] for c in text]\n",
    "\n",
    "idx[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PREFACE\\n\\n\\nSUPPOSING that Truth is a woman--what then? Is there not gro'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(indices_char[i] for i in idx[:70])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Three char model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Create inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Create a list of every 4th character, starting at the 0th, 1st, 2nd, then 3rd characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cs=3\n",
    "c1_dat = [idx[i]   for i in range(0, len(idx)-cs, cs)]\n",
    "c2_dat = [idx[i+1] for i in range(0, len(idx)-cs, cs)]\n",
    "c3_dat = [idx[i+2] for i in range(0, len(idx)-cs, cs)]\n",
    "c4_dat = [idx[i+3] for i in range(0, len(idx)-cs, cs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Our inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x1 = np.stack(c1_dat)\n",
    "x2 = np.stack(c2_dat)\n",
    "x3 = np.stack(c3_dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Our output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y = np.stack(c4_dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The first 4 inputs and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([40, 30, 29,  1]), array([42, 25,  1, 43]), array([29, 27,  1, 45]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1[:4], x2[:4], x3[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30, 29,  1, 40])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200297,), (200297,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick a size for our hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of latent factors to create (i.e. the size of the embedding matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fac = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Char3Model(nn.Module):\n",
    "    def __init__(self, vocab_size, n_fac):\n",
    "        super().__init__()\n",
    "        self.e = nn.Embedding(vocab_size, n_fac)\n",
    "        self.input = nn.Linear(n_fac, n_hidden)\n",
    "        self.hidden = nn.Linear(n_hidden, n_hidden)\n",
    "        self.output = nn.Linear(n_hidden, vocab_size)\n",
    "        \n",
    "    def forward(self, c1, c2, c3):\n",
    "        e1 = self.e(c1)\n",
    "        a1 = F.relu(self.input(e1))\n",
    "        h1 = F.tanh(self.hidden(a1))\n",
    "        \n",
    "        e2 = self.e(c2)\n",
    "        a2 = F.relu(self.input(e2))\n",
    "        h2 = F.tanh(self.hidden(h1+a2))\n",
    "        \n",
    "        e3 = self.e(c3)\n",
    "        a3 = F.relu(self.input(e3))\n",
    "        h3 = F.tanh(self.hidden(h2+a3))\n",
    "        \n",
    "        o1 = self.output(h3)\n",
    "        \n",
    "        return F.log_softmax(o1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = ColumnarModelData.from_arrays('.', [-1], np.stack([x1,x2,x3], axis=1), y, bs=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Char3Model(vocab_size, n_fac).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(md.trn_dl)\n",
    "*xs,yt = next(it)\n",
    "t = m(*V(xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.Adam(m.parameters(), 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac62df327c384a548f7239110a0c10fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                              \n",
      "    0      2.089592   0.355989  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.35598898]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(m, md, 1, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "??fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_lrs(opt, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(m, md, 1, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_next(inp):\n",
    "    idxs = T(np.array([char_indices[c] for c in inp]))\n",
    "    p = m(*VV(idxs))\n",
    "    i = np.argmax(to_np(p))\n",
    "    return chars[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "get_next('y. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "get_next('ppl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "get_next(' th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "get_next('and')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our first RNN!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Create inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This is the size of our unrolled RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cs=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For each of 0 through 7, create a list of every 8th character with that starting point. These will be the 8 inputs to out model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "c_in_dat = [[idx[i+j] for i in range(cs)] for j in range(len(idx)-cs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Then create a list of the next character in each of these series. This will be the labels for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "c_out_dat = [idx[j+cs] for j in range(len(idx)-cs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "xs = np.stack(c_in_dat, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600885, 8)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y = np.stack(c_out_dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "So each column below is one series of 8 characters from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[40, 42, 29, 30, 25, 27, 29,  1],\n",
       "       [42, 29, 30, 25, 27, 29,  1,  1],\n",
       "       [29, 30, 25, 27, 29,  1,  1,  1],\n",
       "       [30, 25, 27, 29,  1,  1,  1, 43],\n",
       "       [25, 27, 29,  1,  1,  1, 43, 45],\n",
       "       [27, 29,  1,  1,  1, 43, 45, 40],\n",
       "       [29,  1,  1,  1, 43, 45, 40, 40],\n",
       "       [ 1,  1,  1, 43, 45, 40, 40, 39]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs[:cs,:cs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "...and this is the next character after each sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  1, 43, 45, 40, 40, 39, 43])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:cs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_idx = get_cv_idxs(len(idx)-cs-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = ColumnarModelData.from_arrays('.', val_idx, xs, y, bs=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_batch = next(iter(md.trn_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_batch[0].size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "e = torch.nn.Embedding(vocab_size, n_fac)\n",
    "inp = torch.nn.Linear(n_fac, n_hidden)\n",
    "hidden = torch.nn.Linear(n_hidden, n_hidden)\n",
    "output = torch.nn.Linear(n_hidden, cs)\n",
    "\n",
    "\n",
    "# bs = cs[0].size(0)\n",
    "# h = V(torch.zeros(bs, n_hidden).cuda())\n",
    "# #         h = torch.autograd.Variable(torch.zeros((n_hidden))).cuda()\n",
    "# for c in cs:\n",
    "#     e1 = self.e(c)\n",
    "#     in1 = F.relu(self.input(e1))\n",
    "#     h = F.tanh(self.hidden(h+in1))\n",
    "# o = self.output(h)\n",
    "# res = F.log_softmax(o, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharLoopModel(nn.Module):\n",
    "    # This is an RNN!\n",
    "    def __init__(self, vocab_size, n_fac):\n",
    "        super().__init__()\n",
    "        self.e = torch.nn.Embedding(vocab_size, n_fac)\n",
    "        self.input = torch.nn.Linear(n_fac, n_hidden)\n",
    "        self.hidden = torch.nn.Linear(n_hidden, n_hidden)\n",
    "        self.output = torch.nn.Linear(n_hidden, vocab_size)\n",
    "        \n",
    "    def forward(self, *cs):\n",
    "        bs = cs[0].size(0)\n",
    "        h = V(torch.zeros(bs, n_hidden).cuda())\n",
    "#         h = torch.autograd.Variable(torch.zeros((n_hidden))).cuda()\n",
    "        for c in cs:\n",
    "            e1 = self.e(c)\n",
    "            in1 = F.relu(self.input(e1))\n",
    "            h = F.tanh(self.hidden(h+in1))\n",
    "        o = self.output(h)\n",
    "        return F.log_softmax(o, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = CharLoopModel(vocab_size, n_fac).cuda()\n",
    "opt = optim.Adam(m.parameters(), 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c070515a3374035a41faaa1ec060d6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/939 [00:00<?, ?it/s]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  0%|          | 1/939 [00:00<06:06,  2.56it/s, loss=4.5]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  0%|          | 1/939 [00:00<06:27,  2.42it/s, loss=4.07]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  0%|          | 1/939 [00:00<06:37,  2.36it/s, loss=3.74]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  0%|          | 1/939 [00:00<06:59,  2.24it/s, loss=3.64]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  1%|          | 5/939 [00:00<01:33,  9.98it/s, loss=3.53]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  1%|          | 5/939 [00:00<01:39,  9.36it/s, loss=3.48]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  1%|          | 5/939 [00:00<01:48,  8.64it/s, loss=3.4] torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  1%|          | 8/939 [00:00<01:11, 12.96it/s, loss=3.35]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  1%|          | 8/939 [00:00<01:16, 12.11it/s, loss=3.3] torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  1%|          | 8/939 [00:00<01:20, 11.52it/s, loss=3.26]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  1%|          | 11/939 [00:00<01:06, 13.93it/s, loss=3.22]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  1%|          | 11/939 [00:00<01:07, 13.73it/s, loss=3.18]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  1%|          | 11/939 [00:00<01:09, 13.35it/s, loss=3.15]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  1%|          | 11/939 [00:00<01:12, 12.75it/s, loss=3.12]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  2%|▏         | 15/939 [00:00<00:56, 16.34it/s, loss=3.1] torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  2%|▏         | 15/939 [00:00<01:00, 15.16it/s, loss=3.08]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  2%|▏         | 15/939 [00:01<01:06, 13.99it/s, loss=3.04]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  2%|▏         | 18/939 [00:01<00:57, 16.04it/s, loss=3.03]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  2%|▏         | 18/939 [00:01<01:00, 15.24it/s, loss=3]   torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  2%|▏         | 20/939 [00:01<00:57, 16.11it/s, loss=2.98]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  2%|▏         | 20/939 [00:01<00:59, 15.38it/s, loss=2.97]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  2%|▏         | 22/939 [00:01<00:56, 16.28it/s, loss=2.95]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  2%|▏         | 22/939 [00:01<00:59, 15.47it/s, loss=2.93]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  3%|▎         | 24/939 [00:01<00:55, 16.48it/s, loss=2.92]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  3%|▎         | 24/939 [00:01<00:57, 15.81it/s, loss=2.9] torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  3%|▎         | 26/939 [00:01<00:55, 16.45it/s, loss=2.89]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  3%|▎         | 26/939 [00:01<00:57, 15.84it/s, loss=2.88]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  3%|▎         | 28/939 [00:01<00:55, 16.40it/s, loss=2.87]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  3%|▎         | 28/939 [00:01<00:57, 15.90it/s, loss=2.85]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  3%|▎         | 30/939 [00:01<00:54, 16.54it/s, loss=2.84]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  3%|▎         | 30/939 [00:01<00:56, 16.07it/s, loss=2.83]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  3%|▎         | 30/939 [00:01<00:57, 15.69it/s, loss=2.82]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  4%|▎         | 33/939 [00:01<00:53, 16.99it/s, loss=2.81]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 33/939 [00:01<00:54, 16.71it/s, loss=2.8] torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  4%|▎         | 33/939 [00:02<00:55, 16.37it/s, loss=2.8]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  4%|▍         | 36/939 [00:02<00:51, 17.58it/s, loss=2.79]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  4%|▍         | 36/939 [00:02<00:51, 17.48it/s, loss=2.78]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  4%|▍         | 36/939 [00:02<00:52, 17.34it/s, loss=2.76]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  4%|▍         | 36/939 [00:02<00:52, 17.22it/s, loss=2.75]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  4%|▍         | 36/939 [00:02<00:52, 17.08it/s, loss=2.74]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  4%|▍         | 41/939 [00:02<00:47, 18.98it/s, loss=2.73]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  4%|▍         | 41/939 [00:02<00:48, 18.36it/s, loss=2.72]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  4%|▍         | 41/939 [00:02<00:50, 17.89it/s, loss=2.72]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  5%|▍         | 44/939 [00:02<00:47, 18.71it/s, loss=2.71]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  5%|▍         | 44/939 [00:02<00:49, 18.21it/s, loss=2.7] torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  5%|▍         | 44/939 [00:02<00:50, 17.83it/s, loss=2.69]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  5%|▌         | 47/939 [00:02<00:49, 18.20it/s, loss=2.69]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  5%|▌         | 47/939 [00:02<00:49, 18.13it/s, loss=2.68]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  5%|▌         | 47/939 [00:02<00:49, 17.94it/s, loss=2.67]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  5%|▌         | 47/939 [00:02<00:49, 17.87it/s, loss=2.67]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  5%|▌         | 47/939 [00:02<00:50, 17.58it/s, loss=2.66]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  6%|▌         | 52/939 [00:02<00:45, 19.28it/s, loss=2.66]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  6%|▌         | 52/939 [00:02<00:46, 19.15it/s, loss=2.65]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  6%|▌         | 52/939 [00:02<00:46, 19.09it/s, loss=2.64]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  6%|▌         | 52/939 [00:02<00:46, 18.93it/s, loss=2.64]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  6%|▌         | 52/939 [00:02<00:47, 18.87it/s, loss=2.63]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  6%|▌         | 52/939 [00:02<00:47, 18.74it/s, loss=2.63]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  6%|▌         | 52/939 [00:02<00:47, 18.64it/s, loss=2.62]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  6%|▋         | 59/939 [00:02<00:41, 20.98it/s, loss=2.61]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  6%|▋         | 59/939 [00:02<00:42, 20.49it/s, loss=2.6] torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  6%|▋         | 59/939 [00:02<00:44, 20.00it/s, loss=2.6]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  6%|▋         | 59/939 [00:03<00:44, 19.62it/s, loss=2.6]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  7%|▋         | 63/939 [00:03<00:42, 20.51it/s, loss=2.59]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  7%|▋         | 63/939 [00:03<00:43, 20.02it/s, loss=2.59]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  7%|▋         | 63/939 [00:03<00:44, 19.68it/s, loss=2.58]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  7%|▋         | 63/939 [00:03<00:45, 19.30it/s, loss=2.57]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  7%|▋         | 67/939 [00:03<00:43, 20.17it/s, loss=2.57]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 67/939 [00:03<00:43, 20.01it/s, loss=2.56]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  7%|▋         | 67/939 [00:03<00:43, 19.88it/s, loss=2.56]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  7%|▋         | 67/939 [00:03<00:44, 19.74it/s, loss=2.55]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  7%|▋         | 67/939 [00:03<00:44, 19.67it/s, loss=2.55]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  8%|▊         | 72/939 [00:03<00:41, 21.01it/s, loss=2.54]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  8%|▊         | 72/939 [00:03<00:41, 20.96it/s, loss=2.54]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  8%|▊         | 72/939 [00:03<00:41, 20.83it/s, loss=2.54]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  8%|▊         | 72/939 [00:03<00:41, 20.71it/s, loss=2.54]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  8%|▊         | 72/939 [00:03<00:41, 20.65it/s, loss=2.54]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  8%|▊         | 72/939 [00:03<00:42, 20.61it/s, loss=2.53]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  8%|▊         | 72/939 [00:03<00:42, 20.51it/s, loss=2.53]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  8%|▊         | 72/939 [00:03<00:42, 20.46it/s, loss=2.52]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  9%|▊         | 80/939 [00:03<00:37, 22.61it/s, loss=2.52]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  9%|▊         | 80/939 [00:03<00:38, 22.56it/s, loss=2.52]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  9%|▊         | 80/939 [00:03<00:38, 22.41it/s, loss=2.51]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  9%|▊         | 80/939 [00:03<00:38, 22.34it/s, loss=2.51]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  9%|▊         | 80/939 [00:03<00:38, 22.20it/s, loss=2.5] torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  9%|▊         | 80/939 [00:03<00:38, 22.06it/s, loss=2.5]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  9%|▊         | 80/939 [00:03<00:39, 22.01it/s, loss=2.5]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  9%|▉         | 87/939 [00:03<00:35, 23.80it/s, loss=2.49]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  9%|▉         | 87/939 [00:03<00:35, 23.73it/s, loss=2.48]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  9%|▉         | 87/939 [00:03<00:36, 23.60it/s, loss=2.48]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  9%|▉         | 87/939 [00:03<00:36, 23.51it/s, loss=2.48]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  9%|▉         | 87/939 [00:03<00:36, 23.39it/s, loss=2.47]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "  9%|▉         | 87/939 [00:03<00:36, 23.31it/s, loss=2.47]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      " 10%|▉         | 93/939 [00:03<00:34, 24.75it/s, loss=2.47]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      " 10%|▉         | 93/939 [00:03<00:34, 24.64it/s, loss=2.46]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      " 10%|▉         | 93/939 [00:03<00:34, 24.50it/s, loss=2.46]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      " 10%|▉         | 93/939 [00:03<00:34, 24.36it/s, loss=2.46]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      " 10%|▉         | 93/939 [00:03<00:34, 24.23it/s, loss=2.45]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      " 10%|▉         | 93/939 [00:03<00:35, 24.13it/s, loss=2.45]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      " 11%|█         | 99/939 [00:03<00:32, 25.55it/s, loss=2.45]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      " 11%|█         | 99/939 [00:03<00:33, 25.43it/s, loss=2.44]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      " 11%|█         | 99/939 [00:03<00:33, 25.29it/s, loss=2.44]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 11%|█         | 99/939 [00:03<00:33, 25.17it/s, loss=2.44]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      " 11%|█         | 99/939 [00:03<00:33, 25.00it/s, loss=2.43]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      " 11%|█         | 99/939 [00:03<00:33, 24.78it/s, loss=2.43]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      " 11%|█         | 105/939 [00:04<00:32, 26.03it/s, loss=2.43]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      " 11%|█         | 105/939 [00:04<00:32, 25.78it/s, loss=2.42]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      " 11%|█         | 105/939 [00:04<00:32, 25.56it/s, loss=2.42]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      " 11%|█         | 105/939 [00:04<00:32, 25.35it/s, loss=2.42]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      " 11%|█         | 105/939 [00:04<00:33, 25.19it/s, loss=2.42]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      " 12%|█▏        | 110/939 [00:04<00:31, 26.20it/s, loss=2.42]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      " 12%|█▏        | 110/939 [00:04<00:31, 25.96it/s, loss=2.42]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      " 12%|█▏        | 110/939 [00:04<00:32, 25.57it/s, loss=2.42]torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-8e1cb7e8a6e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/fastai/courses/dl1/fastai/model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, data, epochs, opt, crit, metrics, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mbatch_num\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstepper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mavg_mom\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mavg_mom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mdebias_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mavg_mom\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fastai/courses/dl1/fastai/model.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, xs, y)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreg_fn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreg_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxtra\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0;31m# Gradient clipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainable_params_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fit(m, md, 1, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_lrs(opt, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(m, md, 1, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "    1     1     1     1\n",
       "    1     1     1     1\n",
       "    1     1     1     1\n",
       "    1     1     1     1\n",
       "    1     1     1     1\n",
       "    1     1     1     1\n",
       "    1     1     1     1\n",
       "    1     1     1     1\n",
       "    1     1     1     1\n",
       "    1     1     1     1\n",
       "[torch.FloatTensor of size 10x4]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((torch.ones((10, 2)), torch.ones((10, 2))), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastAi original: Concat embedding with hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharLoopConcatModel(nn.Module):\n",
    "    def __init__(self, vocab_size, n_fac):\n",
    "        super().__init__()\n",
    "        self.e = torch.nn.Embedding(vocab_size, n_fac)\n",
    "        self.input = torch.nn.Linear(n_fac+n_hidden, n_hidden)\n",
    "        self.hidden = torch.nn.Linear(n_hidden, n_hidden)\n",
    "        self.output = torch.nn.Linear(n_hidden, vocab_size)\n",
    "        \n",
    "    def forward(self, *cs):\n",
    "        bs = cs[0].size(0)\n",
    "        h = torch.autograd.Variable(torch.zeros(bs, n_hidden).cuda())\n",
    "        for c in cs:\n",
    "            e1 = self.e(c)\n",
    "            cat = torch.cat([h, e1], dim=1)\n",
    "            in1 = F.relu(self.input(cat))\n",
    "            h = F.tanh(self.hidden(in1))\n",
    "        o = self.output(h)\n",
    "        return F.log_softmax(o, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = CharLoopConcatModel(vocab_size, n_fac).cuda()\n",
    "opt = optim.Adam(m.parameters(), 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(md.trn_dl)\n",
    "*xs,yt = next(it)\n",
    "t = m(*V(xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60a6aa80eb694ec0b340511624904500",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                              \n",
      "    0      1.822172   1.790493  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.7904927]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(m, md, 1, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_lrs(opt, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27e112d03def4701baeb13add4f34f7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                              \n",
      "    0      1.704583   1.710664  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.7106643]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(m, md, 1, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Model: concat hidden with hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharLoopConcatModel(nn.Module):\n",
    "    def __init__(self, vocab_size, n_fac):\n",
    "        super().__init__()\n",
    "        self.e = torch.nn.Embedding(vocab_size, n_fac)\n",
    "        self.input = torch.nn.Linear(n_fac, n_hidden)\n",
    "        self.hidden = torch.nn.Linear(n_hidden*2, n_hidden)\n",
    "        self.output = torch.nn.Linear(n_hidden, vocab_size)\n",
    "        \n",
    "    def forward(self, *cs):\n",
    "        bs = cs[0].size(0)\n",
    "        h = torch.autograd.Variable(torch.zeros(bs, n_hidden).cuda())\n",
    "        for c in cs:\n",
    "            e1 = self.e(c)\n",
    "            in1 = F.relu(self.input(e1))\n",
    "            cat = torch.cat([h, in1], dim=1)\n",
    "            h = F.tanh(self.hidden(cat))\n",
    "        o = self.output(h)\n",
    "        return F.log_softmax(o, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = CharLoopConcatModel(vocab_size, n_fac).cuda()\n",
    "opt = optim.Adam(m.parameters(), 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(md.trn_dl)\n",
    "*xs,yt = next(it)\n",
    "t = m(*V(xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55ee3598c0fc40949443732231d9b2f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                              \n",
      "    0      1.811987   1.796126  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.7961255]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(m, md, 1, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_lrs(opt, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4846fe4d43f43e4849d3d3acc996190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                              \n",
      "    0      1.730774   1.731683  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.7316834]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(m, md, 1, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_next(inp):\n",
    "    idxs = T(np.array([char_indices[c] for c in inp]))\n",
    "    p = m(*VV(idxs))\n",
    "    i = np.argmax(to_np(p))\n",
    "    return chars[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next('for thos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next('part of ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next('queens a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## RNN with pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Andrew playing around with dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "teste = nn.Embedding(vocab_size, n_fac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "*testx, testy = next(iter(md.trn_dl))\n",
    "testx1 = [c.cpu() for c in testx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "testy = torch.autograd.Variable(testy.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack1 = torch.stack(testx1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 512, 42])"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testinp = teste(torch.autograd.Variable((stack1)))\n",
    "testinp.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "testrnn = nn.RNN(n_fac, n_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = testx1[0].size(0)\n",
    "testh = torch.autograd.Variable(torch.zeros((1, bs, n_hidden)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "outp, h = testrnn(testinp, testh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 512, 256])"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outp.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 256])"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "testout = nn.Linear(n_hidden, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = F.log_softmax(testout(outp[-1]))\n",
    "res = F.log_softmax(testout(outp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Res size: torch.Size([8, 512, 85])\n",
      "Y size: torch.Size([512, 8])\n"
     ]
    }
   ],
   "source": [
    "print('Res size:', res.size())\n",
    "print('Y size:', testy.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2 or 4 dimensions (got 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-336-0dc8b184fd42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce)\u001b[0m\n\u001b[1;32m   1138\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m     \"\"\"\n\u001b[0;32m-> 1140\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce)\u001b[0m\n\u001b[1;32m   1051\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1053\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Expected 2 or 4 dimensions (got {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2 or 4 dimensions (got 3)"
     ]
    }
   ],
   "source": [
    "F.cross_entropy(res, testy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is testing for output model. Y size should be torch.Size([512, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 4.4597\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.nll_loss(res, testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sl,bs,nh = res.size() # sequence length (# of timestamps), batch_size, hidden state\n",
    "sl,bs,vs = res.size() # sequence length (# of timestamps), batch_size, vocab_size\n",
    "# I believe vc should actually be batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096])"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_testy = testy.transpose(0,1).contiguous().view(-1); flat_testy.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096, 85])"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_res = res.view(-1, vs); flat_res.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 2.0824\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def stack_nll_loss(inp, targ):\n",
    "F.nll_loss(flat_res, flat_testy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actial rnn code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRnn(nn.Module):\n",
    "    def __init__(self, vocab_size, n_fac):\n",
    "        super().__init__()\n",
    "        self.e = nn.Embedding(vocab_size, n_fac)\n",
    "        self.rnn = nn.RNN(n_fac, n_hidden)\n",
    "        self.out = nn.Linear(n_hidden, vocab_size)\n",
    "        \n",
    "    def forward(self, *cs):\n",
    "        bs = cs[0].size(0)\n",
    "        h = torch.autograd.Variable(torch.zeros((1, bs, n_hidden)).cuda())\n",
    "        e1 = self.e(torch.stack(cs))\n",
    "        outp, h = self.rnn(e1, h)\n",
    "        return F.log_softmax(self.out(outp[-1]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = CharRnn(vocab_size, n_fac).cuda()\n",
    "opt = optim.Adam(m.parameters(), 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(md.trn_dl)\n",
    "*xs,yt = next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 512, 42])"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = m.e(V(torch.stack(xs)))\n",
    "t.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 512, 256]), torch.Size([1, 512, 256]))"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ht = V(torch.zeros(1, 512,n_hidden))\n",
    "outp, hn = m.rnn(t, ht)\n",
    "outp.size(), hn.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 85])"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = m(*V(xs)); t.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(m, md, 4, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_lrs(opt, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(m, md, 2, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_next(inp):\n",
    "    idxs = T(np.array([char_indices[c] for c in inp]))\n",
    "    p = m(*VV(idxs))\n",
    "    i = np.argmax(to_np(p))\n",
    "    return chars[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "get_next('for thos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_next_n(inp, n):\n",
    "    res = inp\n",
    "    for i in range(n):\n",
    "        c = get_next(inp)\n",
    "        res += c\n",
    "        inp = inp[1:]+c\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "get_next_n('for thos', 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Multi-output model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's take non-overlapping sets of characters this time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "c_in_dat = [[idx[i+j] for i in range(cs)] for j in range(0, len(idx)-cs-1, cs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Then create the exact same thing, offset by 1, as our labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "c_out_dat = [[idx[i+j] for i in range(cs)] for j in range(1, len(idx)-cs, cs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75111, 8)"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs = np.stack(c_in_dat)\n",
    "xs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75111, 8)"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys = np.stack(c_out_dat)\n",
    "ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[40, 42, 29, 30, 25, 27, 29,  1],\n",
       "       [ 1,  1, 43, 45, 40, 40, 39, 43],\n",
       "       [33, 38, 31,  2, 73, 61, 54, 73],\n",
       "       [ 2, 44, 71, 74, 73, 61,  2, 62],\n",
       "       [72,  2, 54,  2, 76, 68, 66, 54],\n",
       "       [67,  9,  9, 76, 61, 54, 73,  2],\n",
       "       [73, 61, 58, 67, 24,  2, 33, 72],\n",
       "       [ 2, 73, 61, 58, 71, 58,  2, 67]])"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs[:cs,:cs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[42, 29, 30, 25, 27, 29,  1,  1],\n",
       "       [ 1, 43, 45, 40, 40, 39, 43, 33],\n",
       "       [38, 31,  2, 73, 61, 54, 73,  2],\n",
       "       [44, 71, 74, 73, 61,  2, 62, 72],\n",
       "       [ 2, 54,  2, 76, 68, 66, 54, 67],\n",
       "       [ 9,  9, 76, 61, 54, 73,  2, 73],\n",
       "       [61, 58, 67, 24,  2, 33, 72,  2],\n",
       "       [73, 61, 58, 71, 58,  2, 67, 68]])"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys[:cs,:cs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_idx = get_cv_idxs(len(xs)-cs-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = ColumnarModelData.from_arrays('.', val_idx, xs, ys, bs=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharSeqRnn(nn.Module):\n",
    "    def __init__(self, vocab_size, n_fac):\n",
    "        super().__init__()\n",
    "        self.e = nn.Embedding(vocab_size, n_fac)\n",
    "        self.rnn = nn.RNN(n_fac, n_hidden)\n",
    "        self.l_out = nn.Linear(n_hidden, vocab_size)\n",
    "        \n",
    "    def forward(self, *cs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = CharSeqRnn(vocab_size, n_fac).cuda()\n",
    "opt = optim.Adam(m.parameters(), 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(md.trn_dl)\n",
    "*xst,yt = next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_loss_seq(inp, targ):\n",
    "    sl,bs,nh = inp.size()\n",
    "    targ = targ.transpose(0,1).contiguous().view(-1)\n",
    "    return F.nll_loss(inp.view(-1,nh), targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fit(m, md, 4, opt, nll_loss_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_lrs(opt, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(m, md, 1, opt, nll_loss_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Identity init!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "m = CharSeqRnn(vocab_size, n_fac).cuda()\n",
    "opt = optim.Adam(m.parameters(), 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "m.rnn.weight_hh_l0.data.copy_(torch.eye(n_hidden))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fit(m, md, 4, opt, nll_loss_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "set_lrs(opt, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fit(m, md, 4, opt, nll_loss_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stateful model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nietzsche.txt  \u001b[0m\u001b[01;34mtrn\u001b[0m/  \u001b[01;34mval\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "from torchtext import vocab, data\n",
    "\n",
    "from fastai.nlp import *\n",
    "from fastai.lm_rnn import *\n",
    "\n",
    "PATH='data/nietzsche/'\n",
    "\n",
    "TRN_PATH = 'trn/'\n",
    "VAL_PATH = 'val/'\n",
    "TRN = f'{PATH}{TRN_PATH}'\n",
    "VAL = f'{PATH}{VAL_PATH}'\n",
    "\n",
    "%ls {PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wc -l nietzsche.txt\n",
    "head -n 8000 nietzsche.txt > trn/nietzsche.txt\n",
    "tail -n 1934 nietzsche.txt > val/nietzsche.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nietzsche.txt\r\n"
     ]
    }
   ],
   "source": [
    "%ls {PATH}trn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(947, 55, 1, 485751)"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT = data.Field(lower=True, tokenize=list)\n",
    "bs=64; bptt=8; n_fac=42; n_hidden=256\n",
    "\n",
    "FILES = dict(train=TRN_PATH, validation=VAL_PATH, test=VAL_PATH)\n",
    "md = LanguageModelData.from_text_files(PATH, TEXT, **FILES, bs=bs, bptt=bptt, min_freq=3)\n",
    "\n",
    "len(md.trn_dl), md.nt, len(md.trn_ds), len(md.trn_ds[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = next(iter(md.trn_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 64])\n",
      "torch.Size([640])\n"
     ]
    }
   ],
   "source": [
    "for b in b1:\n",
    "    print(b.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "948.732421875"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(md.trn_ds[0].text) / bs) / bptt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "947 - number_tokens / batch_size / bptt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharSeqStatefulRnn(nn.Module):\n",
    "    def __init__(self, vocab_size, n_fac, bs):\n",
    "        self.vocab_size = vocab_size\n",
    "        super().__init__()\n",
    "        self.e = nn.Embedding(vocab_size, n_fac)\n",
    "        self.rnn = nn.RNN(n_fac, n_hidden)\n",
    "        self.l_out = nn.Linear(n_hidden, vocab_size)\n",
    "        self.init_hidden(bs)\n",
    "        \n",
    "    def forward(self, cs):\n",
    "        bs = cs[0].size(0)\n",
    "        if self.h.size(1) != bs: self.init_hidden(bs)\n",
    "        outp,h = self.rnn(self.e(cs), self.h)\n",
    "        self.h = repackage_var(h)\n",
    "        return F.log_softmax(self.l_out(outp), dim=-1).view(-1, self.vocab_size)\n",
    "    \n",
    "    def init_hidden(self, bs): self.h = V(torch.zeros(1, bs, n_hidden))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = CharSeqStatefulRnn(md.nt, n_fac, 512).cuda()\n",
    "opt = optim.Adam(m.parameters(), 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac08319c67f546b3a566c9b1ada4819a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                               \n",
      "    0      1.859614   1.853634  \n",
      "    1      1.688205   1.70605                                \n",
      "    2      1.602531   1.630715                               \n",
      "    3      1.548255   1.600085                               \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.6000848]"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(m, md, 4, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_lrs(opt, 1e-4)\n",
    "\n",
    "fit(m, md, 4, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the pytorch source\n",
    "\n",
    "def RNNCell(input, hidden, w_ih, w_hh, b_ih, b_hh):\n",
    "    return F.tanh(F.linear(input, w_ih, b_ih) + F.linear(hidden, w_hh, b_hh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharSeqStatefulRnn2(nn.Module):\n",
    "    def __init__(self, vocab_size, n_fac, bs):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.e = nn.Embedding(vocab_size, n_fac)\n",
    "        self.rnn = nn.RNNCell(n_fac, n_hidden)\n",
    "        self.l_out = nn.Linear(n_hidden, vocab_size)\n",
    "        self.init_hidden(bs)\n",
    "        \n",
    "    def forward(self, cs):\n",
    "        bs = cs[0].size(0)\n",
    "        if self.h.size(1) != bs: self.init_hidden(bs)\n",
    "        outp = []\n",
    "        o = self.h\n",
    "        for c in cs: \n",
    "            o = self.rnn(self.e(c), o)\n",
    "            outp.append(o)\n",
    "        outp = self.l_out(torch.stack(outp))\n",
    "        self.h = repackage_var(o)\n",
    "        return F.log_softmax(outp, dim=-1).view(-1, self.vocab_size)\n",
    "    \n",
    "    def init_hidden(self, bs): self.h = V(torch.zeros(1, bs, n_hidden))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = CharSeqStatefulRnn2(md.nt, n_fac, 512).cuda()\n",
    "opt = optim.Adam(m.parameters(), 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fit(m, md, 4, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharSeqStatefulGRU(nn.Module):\n",
    "    def __init__(self, vocab_size, n_fac, bs):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.e = nn.Embedding(vocab_size, n_fac)\n",
    "        self.rnn = nn.GRU(n_fac, n_hidden)\n",
    "        self.l_out = nn.Linear(n_hidden, vocab_size)\n",
    "        self.init_hidden(bs)\n",
    "        \n",
    "    def forward(self, cs):\n",
    "        bs = cs[0].size(0)\n",
    "        if self.h.size(1) != bs: self.init_hidden(bs)\n",
    "        outp,h = self.rnn(self.e(cs), self.h)\n",
    "        self.h = repackage_var(h)\n",
    "        return F.log_softmax(self.l_out(outp), dim=-1).view(-1, self.vocab_size)\n",
    "    \n",
    "    def init_hidden(self, bs): self.h = V(torch.zeros(1, bs, n_hidden))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the pytorch source code - for reference\n",
    "\n",
    "def GRUCell(input, hidden, w_ih, w_hh, b_ih, b_hh):\n",
    "    gi = F.linear(input, w_ih, b_ih)\n",
    "    gh = F.linear(hidden, w_hh, b_hh)\n",
    "    i_r, i_i, i_n = gi.chunk(3, 1)\n",
    "    h_r, h_i, h_n = gh.chunk(3, 1)\n",
    "\n",
    "    resetgate = F.sigmoid(i_r + h_r)\n",
    "    inputgate = F.sigmoid(i_i + h_i)\n",
    "    newgate = F.tanh(i_n + resetgate * h_n)\n",
    "    return newgate + inputgate * (hidden - newgate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = CharSeqStatefulGRU(md.nt, n_fac, 512).cuda()\n",
    "\n",
    "opt = optim.Adam(m.parameters(), 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fit(m, md, 6, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_lrs(opt, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fit(m, md, 3, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together: LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import sgdr\n",
    "\n",
    "n_hidden=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharSeqStatefulLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, n_fac, bs, nl):\n",
    "        super().__init__()\n",
    "        self.vocab_size,self.nl = vocab_size,nl\n",
    "        self.e = nn.Embedding(vocab_size, n_fac)\n",
    "        self.rnn = nn.LSTM(n_fac, n_hidden, nl, dropout=0.5)\n",
    "        self.l_out = nn.Linear(n_hidden, vocab_size)\n",
    "        self.init_hidden(bs)\n",
    "        \n",
    "    def forward(self, cs):\n",
    "        bs = cs[0].size(0)\n",
    "        if self.h[0].size(1) != bs: self.init_hidden(bs)\n",
    "        outp,h = self.rnn(self.e(cs), self.h)\n",
    "        self.h = repackage_var(h)\n",
    "        return F.log_softmax(self.l_out(outp), dim=-1).view(-1, self.vocab_size)\n",
    "    \n",
    "    def init_hidden(self, bs):\n",
    "        self.h = (V(torch.zeros(self.nl, bs, n_hidden)),\n",
    "                  V(torch.zeros(self.nl, bs, n_hidden)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = CharSeqStatefulLSTM(md.nt, n_fac, 512, 2).cuda()\n",
    "lo = LayerOptimizer(optim.Adam, m, 1e-2, 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f'{PATH}models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(m, md, 2, lo.opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "on_end = lambda sched, cycle: save_model(m, f'{PATH}models/cyc_{cycle}')\n",
    "cb = [CosAnneal(lo, len(md.trn_dl), cycle_mult=2, on_cycle_end=on_end)]\n",
    "fit(m, md, 2**4-1, lo.opt, F.nll_loss, callbacks=cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "on_end = lambda sched, cycle: save_model(m, f'{PATH}models/cyc_{cycle}')\n",
    "cb = [CosAnneal(lo, len(md.trn_dl), cycle_mult=2, on_cycle_end=on_end)]\n",
    "fit(m, md, 2**6-1, lo.opt, F.nll_loss, callbacks=cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next(inp):\n",
    "    idxs = TEXT.numericalize(inp)\n",
    "    p = m(VV(idxs.transpose(0,1)))\n",
    "    r = torch.multinomial(p[-1].exp(), 1)\n",
    "    return TEXT.vocab.itos[to_np(r)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "get_next('for thos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_n(inp, n):\n",
    "    res = inp\n",
    "    for i in range(n):\n",
    "        c = get_next(inp)\n",
    "        res += c\n",
    "        inp = inp[1:]+c\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_next_n('for thos', 400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "nav_menu": {},
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "216px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
