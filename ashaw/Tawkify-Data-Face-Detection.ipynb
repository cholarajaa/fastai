{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put these at the top of every notebook, to get automatic reloading and inline plotting\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file contains all the main external libs we'll use\n",
    "from fastai.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.transforms import *\n",
    "from fastai.conv_learner import *\n",
    "from fastai.model import *\n",
    "from fastai.dataset import *\n",
    "from fastai.sgdr import *\n",
    "from fastai.plots import *\n",
    "\n",
    "import urllib\n",
    "from PIL import Image\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'data/face/'\n",
    "image_folder = f'{PATH}train/'\n",
    "data_csv = PATH+'tw_dem_images.csv'\n",
    "image_csv = PATH+'image_csv.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(PATH+'Photo_Income_Age_2.8.18.csv')\n",
    "df = pd.read_csv(data_csv, dtype={'local_photo': 'object', 'image_type': 'object'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid = 923214\n",
    "row = df.loc[df.userID == uid].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[row.name]\n",
    "df.at[row.name, 'image_type'] = 'corrupted'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['userID'].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.userID == 922512].image_type.isna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recreating dataset with only the face cropped images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Image Frame with only JPEGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_df = df.loc[(df.local_photo != '') \n",
    "                & (df.local_photo != 'corrupted') \n",
    "                & (df.image_type == 'jpeg') \n",
    "                & ~df.local_photo.isna()\n",
    "               ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_files = glob.glob(image_folder+'*.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_ids = list(map(lambda x: int(x[len(image_folder):-len('.jpg')]), training_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>attractedToGender</th>\n",
       "      <th>fromState</th>\n",
       "      <th>metro_name</th>\n",
       "      <th>User_Age</th>\n",
       "      <th>User_Photo</th>\n",
       "      <th>User_Income</th>\n",
       "      <th>local_photo</th>\n",
       "      <th>image_type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>userID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>992835</th>\n",
       "      <td>female</td>\n",
       "      <td>male</td>\n",
       "      <td>NC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43.0</td>\n",
       "      <td>https://s3-us-west-1.amazonaws.com/tawkifyfile...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>data/tawkify/train/992835.jpg</td>\n",
       "      <td>jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992831</th>\n",
       "      <td>female</td>\n",
       "      <td>male</td>\n",
       "      <td>OH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.0</td>\n",
       "      <td>https://s3-us-west-1.amazonaws.com/tawkifyfile...</td>\n",
       "      <td>40000.0</td>\n",
       "      <td>data/tawkify/train/992831.jpg</td>\n",
       "      <td>jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992820</th>\n",
       "      <td>male</td>\n",
       "      <td>female</td>\n",
       "      <td>TX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49.0</td>\n",
       "      <td>https://s3-us-west-1.amazonaws.com/tawkifyfile...</td>\n",
       "      <td>40000.0</td>\n",
       "      <td>data/tawkify/train/992820.jpg</td>\n",
       "      <td>jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992814</th>\n",
       "      <td>male</td>\n",
       "      <td>female</td>\n",
       "      <td>WA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.0</td>\n",
       "      <td>https://s3-us-west-1.amazonaws.com/tawkifyfile...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>data/tawkify/train/992814.jpg</td>\n",
       "      <td>jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992813</th>\n",
       "      <td>female</td>\n",
       "      <td>male</td>\n",
       "      <td>SC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46.0</td>\n",
       "      <td>https://s3-us-west-1.amazonaws.com/tawkifyfile...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>data/tawkify/train/992813.jpg</td>\n",
       "      <td>jpeg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        gender attractedToGender fromState metro_name  User_Age  \\\n",
       "userID                                                            \n",
       "992835  female              male        NC        NaN      43.0   \n",
       "992831  female              male        OH        NaN      40.0   \n",
       "992820    male            female        TX        NaN      49.0   \n",
       "992814    male            female        WA        NaN      29.0   \n",
       "992813  female              male        SC        NaN      46.0   \n",
       "\n",
       "                                               User_Photo  User_Income  \\\n",
       "userID                                                                   \n",
       "992835  https://s3-us-west-1.amazonaws.com/tawkifyfile...          0.0   \n",
       "992831  https://s3-us-west-1.amazonaws.com/tawkifyfile...      40000.0   \n",
       "992820  https://s3-us-west-1.amazonaws.com/tawkifyfile...      40000.0   \n",
       "992814  https://s3-us-west-1.amazonaws.com/tawkifyfile...          0.0   \n",
       "992813  https://s3-us-west-1.amazonaws.com/tawkifyfile...          0.0   \n",
       "\n",
       "                          local_photo image_type  \n",
       "userID                                            \n",
       "992835  data/tawkify/train/992835.jpg       jpeg  \n",
       "992831  data/tawkify/train/992831.jpg       jpeg  \n",
       "992820  data/tawkify/train/992820.jpg       jpeg  \n",
       "992814  data/tawkify/train/992814.jpg       jpeg  \n",
       "992813  data/tawkify/train/992813.jpg       jpeg  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([992835, 992831, 992820, 992814, 992813, 992810, 992807, 992797,\n",
       "            992796, 992788,\n",
       "            ...\n",
       "              7104,   6835,   6525,   4814,   4035,   3679,   2286,    971,\n",
       "               934,    592],\n",
       "           dtype='int64', name='userID', length=89644)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_df = img_df[img_df.index.isin(list(file_ids))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89644, 9)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58642, 9)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_df.to_csv(image_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_df = pd.read_csv(image_csv, index_col='userID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Facial detection pipeline - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://hackernoon.com/building-a-facial-recognition-pipeline-with-deep-learning-in-tensorflow-66e7645015b8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2015-2016 Carnegie Mellon University\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"Module for dlib-based alignment.\"\"\"\n",
    "\n",
    "# NOTE: This file has been copied from the openface project.\n",
    "#  https://github.com/cmusatyalab/openface/blob/master/openface/align_dlib.py\n",
    "\n",
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "\n",
    "TEMPLATE = np.float32([\n",
    "    (0.0792396913815, 0.339223741112), (0.0829219487236, 0.456955367943),\n",
    "    (0.0967927109165, 0.575648016728), (0.122141515615, 0.691921601066),\n",
    "    (0.168687863544, 0.800341263616), (0.239789390707, 0.895732504778),\n",
    "    (0.325662452515, 0.977068762493), (0.422318282013, 1.04329000149),\n",
    "    (0.531777802068, 1.06080371126), (0.641296298053, 1.03981924107),\n",
    "    (0.738105872266, 0.972268833998), (0.824444363295, 0.889624082279),\n",
    "    (0.894792677532, 0.792494155836), (0.939395486253, 0.681546643421),\n",
    "    (0.96111933829, 0.562238253072), (0.970579841181, 0.441758925744),\n",
    "    (0.971193274221, 0.322118743967), (0.163846223133, 0.249151738053),\n",
    "    (0.21780354657, 0.204255863861), (0.291299351124, 0.192367318323),\n",
    "    (0.367460241458, 0.203582210627), (0.4392945113, 0.233135599851),\n",
    "    (0.586445962425, 0.228141644834), (0.660152671635, 0.195923841854),\n",
    "    (0.737466449096, 0.182360984545), (0.813236546239, 0.192828009114),\n",
    "    (0.8707571886, 0.235293377042), (0.51534533827, 0.31863546193),\n",
    "    (0.516221448289, 0.396200446263), (0.517118861835, 0.473797687758),\n",
    "    (0.51816430343, 0.553157797772), (0.433701156035, 0.604054457668),\n",
    "    (0.475501237769, 0.62076344024), (0.520712933176, 0.634268222208),\n",
    "    (0.565874114041, 0.618796581487), (0.607054002672, 0.60157671656),\n",
    "    (0.252418718401, 0.331052263829), (0.298663015648, 0.302646354002),\n",
    "    (0.355749724218, 0.303020650651), (0.403718978315, 0.33867711083),\n",
    "    (0.352507175597, 0.349987615384), (0.296791759886, 0.350478978225),\n",
    "    (0.631326076346, 0.334136672344), (0.679073381078, 0.29645404267),\n",
    "    (0.73597236153, 0.294721285802), (0.782865376271, 0.321305281656),\n",
    "    (0.740312274764, 0.341849376713), (0.68499850091, 0.343734332172),\n",
    "    (0.353167761422, 0.746189164237), (0.414587777921, 0.719053835073),\n",
    "    (0.477677654595, 0.706835892494), (0.522732900812, 0.717092275768),\n",
    "    (0.569832064287, 0.705414478982), (0.635195811927, 0.71565572516),\n",
    "    (0.69951672331, 0.739419187253), (0.639447159575, 0.805236879972),\n",
    "    (0.576410514055, 0.835436670169), (0.525398405766, 0.841706377792),\n",
    "    (0.47641545769, 0.837505914975), (0.41379548902, 0.810045601727),\n",
    "    (0.380084785646, 0.749979603086), (0.477955996282, 0.74513234612),\n",
    "    (0.523389793327, 0.748924302636), (0.571057789237, 0.74332894691),\n",
    "    (0.672409137852, 0.744177032192), (0.572539621444, 0.776609286626),\n",
    "    (0.5240106503, 0.783370783245), (0.477561227414, 0.778476346951)])\n",
    "\n",
    "INV_TEMPLATE = np.float32([\n",
    "    (-0.04099179660567834, -0.008425234314031194, 2.575498465013183),\n",
    "    (0.04062510634554352, -0.009678089746831375, -1.2534351452524177),\n",
    "    (0.0003666902601348179, 0.01810332406086298, -0.32206331976076663)])\n",
    "\n",
    "TPL_MIN, TPL_MAX = np.min(TEMPLATE, axis=0), np.max(TEMPLATE, axis=0)\n",
    "MINMAX_TEMPLATE = (TEMPLATE - TPL_MIN) / (TPL_MAX - TPL_MIN)\n",
    "\n",
    "\n",
    "class AlignDlib:\n",
    "    \"\"\"\n",
    "    Use `dlib's landmark estimation <http://blog.dlib.net/2014/08/real-time-face-pose-estimation.html>`_ to align faces.\n",
    "    The alignment preprocess faces for input into a neural network.\n",
    "    Faces are resized to the same size (such as 96x96) and transformed\n",
    "    to make landmarks (such as the eyes and nose) appear at the same\n",
    "    location on every image.\n",
    "    Normalized landmarks:\n",
    "    .. image:: ../images/dlib-landmark-mean.png\n",
    "    \"\"\"\n",
    "\n",
    "    #: Landmark indices corresponding to the inner eyes and bottom lip.\n",
    "    INNER_EYES_AND_BOTTOM_LIP = [39, 42, 57]\n",
    "\n",
    "    #: Landmark indices corresponding to the outer eyes and nose.\n",
    "    OUTER_EYES_AND_NOSE = [36, 45, 33]\n",
    "\n",
    "    def __init__(self, facePredictor):\n",
    "        \"\"\"\n",
    "        Instantiate an 'AlignDlib' object.\n",
    "        :param facePredictor: The path to dlib's\n",
    "        :type facePredictor: str\n",
    "        \"\"\"\n",
    "        assert facePredictor is not None\n",
    "\n",
    "        # pylint: disable=no-member\n",
    "        self.detector = dlib.get_frontal_face_detector()\n",
    "        self.predictor = dlib.shape_predictor(facePredictor)\n",
    "\n",
    "    def getAllFaceBoundingBoxes(self, rgbImg):\n",
    "        \"\"\"\n",
    "        Find all face bounding boxes in an image.\n",
    "        :param rgbImg: RGB image to process. Shape: (height, width, 3)\n",
    "        :type rgbImg: numpy.ndarray\n",
    "        :return: All face bounding boxes in an image.\n",
    "        :rtype: dlib.rectangles\n",
    "        \"\"\"\n",
    "        assert rgbImg is not None\n",
    "\n",
    "        try:\n",
    "            return self.detector(rgbImg, 1)\n",
    "        except Exception as e:  # pylint: disable=broad-except\n",
    "            print(\"Warning: {}\".format(e))\n",
    "            # In rare cases, exceptions are thrown.\n",
    "            return []\n",
    "\n",
    "    def getLargestFaceBoundingBox(self, rgbImg, skipMulti=False):\n",
    "        \"\"\"\n",
    "        Find the largest face bounding box in an image.\n",
    "        :param rgbImg: RGB image to process. Shape: (height, width, 3)\n",
    "        :type rgbImg: numpy.ndarray\n",
    "        :param skipMulti: Skip image if more than one face detected.\n",
    "        :type skipMulti: bool\n",
    "        :return: The largest face bounding box in an image, or None.\n",
    "        :rtype: dlib.rectangle\n",
    "        \"\"\"\n",
    "        assert rgbImg is not None\n",
    "\n",
    "        faces = self.getAllFaceBoundingBoxes(rgbImg)\n",
    "        if (not skipMulti and len(faces) > 0) or len(faces) == 1:\n",
    "            return max(faces, key=lambda rect: rect.width() * rect.height())\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def get_crop_rect(self, rgbImg, face_rect):\n",
    "        img_h, img_w, _ = rgbImg.shape\n",
    "        center = face_rect.center()\n",
    "        wx2 = face_rect.width() * 2\n",
    "        left = (center.x - wx2)\n",
    "        right = (center.x + wx2)\n",
    "        top = (center.y - wx2)\n",
    "        bottom = (center.y + wx2)\n",
    "\n",
    "        # if not enough space\n",
    "        # left < 0\n",
    "        # right > w\n",
    "        # top < 0\n",
    "        # bottom > h\n",
    "\n",
    "        # dim - direction\n",
    "        # left = 0 + left(-7) = -7\n",
    "        # right = 200 - right(220) = -20\n",
    "        # top = 0 + top(-23) = -23\n",
    "        # bottom = 300 - bottom(330) = -30\n",
    "\n",
    "        min_dist = min(left, img_w-right, top, img_h-bottom, 0) # min should be 0 if all else positive.\n",
    "        new_wx2 = face_rect.width()*2 + min_dist\n",
    "\n",
    "\n",
    "        left = (center.x - new_wx2)\n",
    "        right = (center.x + new_wx2)\n",
    "        top = (center.y - new_wx2)\n",
    "        bottom = (center.y + new_wx2)\n",
    "        return left, right, top, bottom\n",
    "\n",
    "    def face_crop(self, rgbImg):\n",
    "        bounding_box = self.getLargestFaceBoundingBox(rgbImg, skipMulti=True)\n",
    "        if bounding_box is None or bounding_box.width() < 100: # emojii detection case...\n",
    "            return None\n",
    "        l,r,t,b = self.get_crop_rect(rgbImg, bounding_box)\n",
    "        \n",
    "        return rgbImg[t:b, l:r, :]\n",
    "        \n",
    "\n",
    "    def findLandmarks(self, rgbImg, bb):\n",
    "        \"\"\"\n",
    "        Find the landmarks of a face.\n",
    "        :param rgbImg: RGB image to process. Shape: (height, width, 3)\n",
    "        :type rgbImg: numpy.ndarray\n",
    "        :param bb: Bounding box around the face to find landmarks for.\n",
    "        :type bb: dlib.rectangle\n",
    "        :return: Detected landmark locations.\n",
    "        :rtype: list of (x,y) tuples\n",
    "        \"\"\"\n",
    "        assert rgbImg is not None\n",
    "        assert bb is not None\n",
    "\n",
    "        points = self.predictor(rgbImg, bb)\n",
    "        # return list(map(lambda p: (p.x, p.y), points.parts()))\n",
    "        return [(p.x, p.y) for p in points.parts()]\n",
    "\n",
    "    # pylint: disable=dangerous-default-value\n",
    "    def align(self, imgDim, rgbImg, bb=None,\n",
    "              landmarks=None, landmarkIndices=INNER_EYES_AND_BOTTOM_LIP,\n",
    "              skipMulti=False, scale=1.0):\n",
    "        r\"\"\"align(imgDim, rgbImg, bb=None, landmarks=None, landmarkIndices=INNER_EYES_AND_BOTTOM_LIP)\n",
    "        Transform and align a face in an image.\n",
    "        :param imgDim: The edge length in pixels of the square the image is resized to.\n",
    "        :type imgDim: int\n",
    "        :param rgbImg: RGB image to process. Shape: (height, width, 3)\n",
    "        :type rgbImg: numpy.ndarray\n",
    "        :param bb: Bounding box around the face to align. \\\n",
    "                   Defaults to the largest face.\n",
    "        :type bb: dlib.rectangle\n",
    "        :param landmarks: Detected landmark locations. \\\n",
    "                          Landmarks found on `bb` if not provided.\n",
    "        :type landmarks: list of (x,y) tuples\n",
    "        :param landmarkIndices: The indices to transform to.\n",
    "        :type landmarkIndices: list of ints\n",
    "        :param skipMulti: Skip image if more than one face detected.\n",
    "        :type skipMulti: bool\n",
    "        :param scale: Scale image before cropping to the size given by imgDim.\n",
    "        :type scale: float\n",
    "        :return: The aligned RGB image. Shape: (imgDim, imgDim, 3)\n",
    "        :rtype: numpy.ndarray\n",
    "        \"\"\"\n",
    "        assert imgDim is not None\n",
    "        assert rgbImg is not None\n",
    "        assert landmarkIndices is not None\n",
    "\n",
    "        if bb is None:\n",
    "            bb = self.getLargestFaceBoundingBox(rgbImg, skipMulti)\n",
    "            if bb is None:\n",
    "                return\n",
    "\n",
    "        if landmarks is None:\n",
    "            landmarks = self.findLandmarks(rgbImg, bb)\n",
    "\n",
    "        npLandmarks = np.float32(landmarks)\n",
    "        npLandmarkIndices = np.array(landmarkIndices)\n",
    "\n",
    "        # pylint: disable=maybe-no-member\n",
    "        H = cv2.getAffineTransform(npLandmarks[npLandmarkIndices],\n",
    "                                   imgDim * MINMAX_TEMPLATE[npLandmarkIndices] * scale + imgDim * (1 - scale) / 2)\n",
    "        thumbnail = cv2.warpAffine(rgbImg, H, (imgDim, imgDim))\n",
    "\n",
    "        return thumbnail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import logging\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "\n",
    "# from medium_facenet_tutorial.align_dlib import AlignDlib\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "align_dlib = AlignDlib(f'{PATH}shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "\n",
    "def preprocess_image(input_path, output_path, crop_dim):\n",
    "    \"\"\"\n",
    "    Detect face, align and crop :param input_path. Write output to :param output_path\n",
    "    :param input_path: Path to input image\n",
    "    :param output_path: Path to write processed image\n",
    "    :param crop_dim: dimensions to crop image to\n",
    "    \"\"\"\n",
    "    image = _process_image(input_path, crop_dim)\n",
    "    if image is not None:\n",
    "        logger.debug('Writing processed file: {}'.format(output_path))\n",
    "        cv2.imwrite(output_path, image)\n",
    "    else:\n",
    "        logger.warning(\"Skipping filename: {}\".format(input_path))\n",
    "\n",
    "\n",
    "def _process_image(filename, crop_dim):\n",
    "    image = None\n",
    "    aligned_image = None\n",
    "    image = _buffer_image(filename)\n",
    "    if image is not None:\n",
    "#         aligned_image = _align_image(image, crop_dim)\n",
    "        cropped_image = _face_crop(image)\n",
    "    else:\n",
    "        raise IOError('Error buffering image: {}'.format(filename))\n",
    "\n",
    "    return cropped_image\n",
    "\n",
    "\n",
    "def _buffer_image(filename):\n",
    "    logger.debug('Reading image: {}'.format(filename))\n",
    "    image = cv2.imread(filename, )\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    return image\n",
    "\n",
    "def _face_crop(image):\n",
    "    face_crop = align_dlib.face_crop(image)\n",
    "#     if face_crop is not None:\n",
    "#         face_crop = cv2.cvtColor(face_crop, cv2.COLOR_BGR2RGB)\n",
    "    return face_crop\n",
    "    \n",
    "\n",
    "def _align_image(image, crop_dim):\n",
    "    bb = align_dlib.getLargestFaceBoundingBox(image)\n",
    "    aligned = align_dlib.align(crop_dim, image, bb, landmarkIndices=AlignDlib.INNER_EYES_AND_BOTTOM_LIP)\n",
    "    if aligned is not None:\n",
    "        aligned = cv2.cvtColor(aligned, cv2.COLOR_BGR2RGB)\n",
    "    return aligned\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     logging.basicConfig(level=logging.INFO)\n",
    "#     parser = argparse.ArgumentParser(add_help=True)\n",
    "#     parser.add_argument('--input-dir', type=str, action='store', default='data', dest='input_dir')\n",
    "#     parser.add_argument('--output-dir', type=str, action='store', default='output', dest='output_dir')\n",
    "#     parser.add_argument('--crop-dim', type=int, action='store', default=180, dest='crop_dim',\n",
    "#                         help='Size to crop images to')\n",
    "\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "#     main(args.input_dir, args.output_dir, args.crop_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_process(input_file):\n",
    "    output_file = input_file.replace('train', 'output')\n",
    "    plt.figure()\n",
    "    plt.imshow(plt.imread(input_file))\n",
    "    preprocess_image(input_file, output_file, 200)\n",
    "    plt.figure()\n",
    "    plt.imshow(plt.imread(output_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files = glob.glob(f'{PATH}train/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_process(input_files[50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process(input_dir, output_dir, crop_dim):\n",
    "#     start_time = time.time()\n",
    "#     pool = mp.Pool(processes=mp.cpu_count())\n",
    "\n",
    "#     if not os.path.exists(output_dir):\n",
    "#         os.makedirs(output_dir)\n",
    "\n",
    "#     for image_dir in os.listdir(input_dir):\n",
    "#         image_output_dir = os.path.join(output_dir, os.path.basename(os.path.basename(image_dir)))\n",
    "#         if not os.path.exists(image_output_dir):\n",
    "#             os.makedirs(image_output_dir)\n",
    "\n",
    "#     image_paths = glob.glob(os.path.join(input_dir, '**/*.jpg'))\n",
    "#     for index, image_path in enumerate(image_paths):\n",
    "#         image_output_dir = os.path.join(output_dir, os.path.basename(os.path.dirname(image_path)))\n",
    "#         output_path = os.path.join(image_output_dir, os.path.basename(image_path))\n",
    "#         pool.apply_async(preprocess_image, (image_path, output_path, crop_dim))\n",
    "\n",
    "#     pool.close()\n",
    "#     pool.join()\n",
    "#     logger.info('Completed in {} seconds'.format(time.time() - start_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(input_dir, output_dir, crop_dim):\n",
    "    start_time = time.time()\n",
    "    pool = mp.Pool(processes=mp.cpu_count())\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    image_paths = glob.glob(os.path.join(input_dir, '*.jpg'))\n",
    "    for index, image_path in enumerate(image_paths):\n",
    "        output_path = image_path.replace(input_dir, output_dir)\n",
    "        pool.apply_async(preprocess_image, (image_path, output_path, crop_dim))\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    logger.info('Completed in {} seconds'.format(time.time() - start_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process('data/tawkify/train/', f'{PATH}train/', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(input_files[12])\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "bbxs = align_dlib.getAllFaceBoundingBoxes(image)\n",
    "fbx = bbxs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# wx2 = (fbx.width() * 2)\n",
    "# left = max(center.x - wx2, 10)\n",
    "# right = min(center.x + wx2, image.shape[1]-10)\n",
    "# top = max(center.y - int(fbx.height()*2), 10)\n",
    "# bottom = min(center.y + int(fbx.height() * 2), image.shape[0]-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cropping alg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crop_rect(image, face_rect):\n",
    "    img_h, img_w, _ = image.shape\n",
    "    center = fbx.center()\n",
    "    wx2 = fbx.width() * 2\n",
    "    left = (center.x - wx2)\n",
    "    right = (center.x + wx2)\n",
    "    top = (center.y - wx2)\n",
    "    bottom = (center.y + wx2)\n",
    "    \n",
    "    # if not enough space\n",
    "    # left < 0\n",
    "    # right > w\n",
    "    # top < 0\n",
    "    # bottom > h\n",
    "\n",
    "    # dim - direction\n",
    "    # left = 0 + left(-7) = -7\n",
    "    # right = 200 - right(220) = -20\n",
    "    # top = 0 + top(-23) = -23\n",
    "    # bottom = 300 - bottom(330) = -30\n",
    "\n",
    "    min_dist = min(left, img_w-right, top, img_h-bottom, 0) # min should be 0 if all else positive.\n",
    "    new_wx2 = fbx.width()*2 + min_dist\n",
    "\n",
    "\n",
    "    left = (center.x - new_wx2)\n",
    "    right = (center.x + new_wx2)\n",
    "    top = (center.y - new_wx2)\n",
    "    bottom = (center.y + new_wx2)\n",
    "    return left, right, top, bottom, (left, top), (right, bottom)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv2.rectangle(image,(fbx.left(),fbx.top()),(fbx.right(),fbx.bottom()),(0,255,0),2)\n",
    "l,r,t,b = get_crop_rect(image, fbx)\n",
    "img_bx = cv2.rectangle(image,(l,t),(r,b),(0,255,0),10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img_bx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_img = image[t:b, l:r, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cropped_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edge cases:  \n",
    "'data/face/train/957569.jpg' -> input_files[12] -> emojii\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
